{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60496c71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-30T16:21:04.921321Z",
     "iopub.status.busy": "2021-09-30T16:21:04.920644Z",
     "iopub.status.idle": "2021-09-30T16:21:12.479945Z",
     "shell.execute_reply": "2021-09-30T16:21:12.479274Z",
     "shell.execute_reply.started": "2021-09-30T16:11:45.401741Z"
    },
    "id": "wlixUSTf1No9",
    "papermill": {
     "duration": 7.648879,
     "end_time": "2021-09-30T16:21:12.480102",
     "exception": false,
     "start_time": "2021-09-30T16:21:04.831223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "pd.set_option('display.max_columns', 100)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler,RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e6db046",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-30T16:21:12.500845Z",
     "iopub.status.busy": "2021-09-30T16:21:12.499127Z",
     "iopub.status.idle": "2021-09-30T16:21:12.598304Z",
     "shell.execute_reply": "2021-09-30T16:21:12.597905Z",
     "shell.execute_reply.started": "2021-09-30T16:11:53.099851Z"
    },
    "id": "WNQU04mT1NpD",
    "outputId": "d39d2b3a-1408-4da2-fccd-6923057a7299",
    "papermill": {
     "duration": 0.109328,
     "end_time": "2021-09-30T16:21:12.598443",
     "exception": false,
     "start_time": "2021-09-30T16:21:12.489115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"train.csv\"\n",
    "#/kaggle/input/dry-beans-classification-iti-ai-pro-intake01/test.csv\n",
    "data_path=\"/kaggle/input/dry-beans-classification-iti-ai-pro-intake01/train.csv\"\n",
    "data = pd.read_csv(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1ef49ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-30T16:21:12.633495Z",
     "iopub.status.busy": "2021-09-30T16:21:12.616142Z",
     "iopub.status.idle": "2021-09-30T16:21:12.643911Z",
     "shell.execute_reply": "2021-09-30T16:21:12.643494Z",
     "shell.execute_reply.started": "2021-09-30T16:11:53.200647Z"
    },
    "papermill": {
     "duration": 0.037695,
     "end_time": "2021-09-30T16:21:12.644033",
     "exception": false,
     "start_time": "2021-09-30T16:21:12.606338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def more_features(df):\n",
    "    df['ShapeFactor5'] = (4*df['Area'])/(df['MinorAxisLength']*df['MinorAxisLength'])\n",
    "     #resources https://en.wikipedia.org/wiki/Eccentricity_(mathematics)\n",
    "    df['Second_Eccentricity'] = df['Eccentricity']/np.sqrt(1-(df['Eccentricity']**2))  \n",
    "    df['Third_Eccentricity'] = df['Eccentricity']/np.sqrt(2-(df['Eccentricity']**2))\n",
    "    df['Angular_Eccentricity'] = np.arcsin(df['Eccentricity'])\n",
    "    \n",
    "    # resources https://en.wikipedia.org/wiki/Sphericity#Ellipsoidal_objects\n",
    "    a = df['MajorAxisLength']\n",
    "    b = df['MinorAxisLength']\n",
    "    df['Sphericity'] = (2*(a*(b**2))**(1/3))/((a)+(b**2)/((a**2)-(b**2))*np.log((((a)+((a**2)-(b**2)))/(b**2))))\n",
    "    \n",
    "    # not implemented https://en.wikipedia.org/wiki/Roundness_(geology)#Measure_of_roundness\n",
    "    ########################################################################################\n",
    "    # https://en.wikipedia.org/wiki/Curvature \n",
    "    df['Curvature_R1'] = (b*b)/a\n",
    "    df['Curvature_R2'] = (a*a)/b\n",
    "    # https://en.wikipedia.org/wiki/Degree_of_curvature#Formula_from_radius\n",
    "    df ['Curvature_Degree_1'] = 5729.58/df['Curvature_R1']\n",
    "    df ['Curvature_Degree_2'] = 5729.58/df['Curvature_R2']\n",
    "    \n",
    "    return df\n",
    "\n",
    "data = more_features(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c83dfcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-30T16:21:12.672253Z",
     "iopub.status.busy": "2021-09-30T16:21:12.662379Z",
     "iopub.status.idle": "2021-09-30T16:22:38.790569Z",
     "shell.execute_reply": "2021-09-30T16:22:38.791160Z",
     "shell.execute_reply.started": "2021-09-30T16:11:53.233956Z"
    },
    "papermill": {
     "duration": 86.139313,
     "end_time": "2021-09-30T16:22:38.791356",
     "exception": false,
     "start_time": "2021-09-30T16:21:12.652043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "cols_cat = ['Area', 'Perimeter', 'MajorAxisLength', 'MinorAxisLength',\n",
    "       #'AspectRation',\n",
    "        'Eccentricity', 'ConvexArea', 'EquivDiameter', 'Extent',\n",
    "       'Solidity', 'roundness', 'Compactness', 'ShapeFactor1', 'ShapeFactor2',\n",
    "       #'ShapeFactor3',\n",
    "        'ShapeFactor4','ShapeFactor5',\n",
    "       #'Second_Eccentricity', \n",
    "        #'Third_Eccentricity',\n",
    "        #'Angular_Eccentricity',\n",
    "       'Sphericity', \n",
    "        'Curvature_R1', 'Curvature_R2', 'Curvature_Degree_1',\n",
    "       'Curvature_Degree_2' ,'y']\n",
    "\n",
    " \n",
    "data_cat =  data[cols_cat]\n",
    "\n",
    "# data=data.drop(\"ID\",axis=1)\n",
    "label_encoder = LabelEncoder()\n",
    "robust_scaler = RobustScaler()\n",
    "\n",
    "#robust_scaler.fit(data)\n",
    "#data=\n",
    "out_cat=label_encoder.fit_transform(data_cat.iloc[:, -1])\n",
    "y_label_classes = label_encoder.classes_\n",
    "n_samples = data_cat.shape[0]\n",
    "\n",
    "X_cat=data_cat.drop([\"y\"],axis=1)\n",
    "y_cat=out_cat\n",
    "\n",
    "X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(X_cat, y_cat, test_size=0.20, random_state=42)\n",
    "\n",
    "X_train_cat=X_train_cat.reset_index(drop=True)\n",
    "\n",
    "X_test_cat=X_test_cat.reset_index(drop=True)\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "clf_cat=CatBoostClassifier(silent=True,depth=5,iterations= 2000,random_state=42)\n",
    "\n",
    "clf_cat = clf_cat.fit(X_train_cat,y_train_cat)\n",
    "\n",
    "\n",
    "y_pred_train_cat = clf_cat.predict(X_train_cat)\n",
    "y_pred_test_cat =  clf_cat.predict(X_test_cat)\n",
    "\n",
    "\n",
    "\n",
    "clf_cat = clf_cat.fit(X_cat, y_cat)\n",
    "y_pred_train_cat = clf_cat.predict(data_cat.drop([\"y\"],axis=1))\n",
    "y_pred_test_cat =  clf_cat.predict(X_test_cat)\n",
    "\n",
    "test_path=\"/kaggle/input/dry-beans-classification-iti-ai-pro-intake01/test.csv\"\n",
    "test_cat = pd.read_csv(test_path)\n",
    "testcopy_cat=test_cat.copy()\n",
    "test_cat=test_cat.drop(\"ID\",axis=1)\n",
    "test_cat = more_features(test_cat)\n",
    "standard_scaler_cat = RobustScaler()\n",
    "\n",
    "y_preds_cat=clf_cat.predict(test_cat)    \n",
    "\n",
    "y_pred_cat=label_encoder.inverse_transform(y_preds_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc6a335c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-30T16:22:38.826372Z",
     "iopub.status.busy": "2021-09-30T16:22:38.810692Z",
     "iopub.status.idle": "2021-09-30T16:24:31.946479Z",
     "shell.execute_reply": "2021-09-30T16:24:31.946884Z",
     "shell.execute_reply.started": "2021-09-30T16:13:23.182136Z"
    },
    "papermill": {
     "duration": 113.146819,
     "end_time": "2021-09-30T16:24:31.947061",
     "exception": false,
     "start_time": "2021-09-30T16:22:38.800242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n",
      "Epoch 0/299\n",
      "----------\n",
      "train Loss: 0.7898 Acc: 0.7392\n",
      "val Loss: 0.3122 Acc: 0.8966\n",
      "\n",
      "Epoch 1/299\n",
      "----------\n",
      "train Loss: 0.2582 Acc: 0.9113\n",
      "val Loss: 0.2296 Acc: 0.9243\n",
      "\n",
      "Epoch 2/299\n",
      "----------\n",
      "train Loss: 0.2313 Acc: 0.9181\n",
      "val Loss: 0.2135 Acc: 0.9257\n",
      "\n",
      "Epoch 3/299\n",
      "----------\n",
      "train Loss: 0.2222 Acc: 0.9204\n",
      "val Loss: 0.2132 Acc: 0.9312\n",
      "\n",
      "Epoch 4/299\n",
      "----------\n",
      "train Loss: 0.2164 Acc: 0.9205\n",
      "val Loss: 0.2017 Acc: 0.9308\n",
      "\n",
      "Epoch 5/299\n",
      "----------\n",
      "train Loss: 0.2151 Acc: 0.9248\n",
      "val Loss: 0.1981 Acc: 0.9359\n",
      "\n",
      "Epoch 6/299\n",
      "----------\n",
      "train Loss: 0.2120 Acc: 0.9230\n",
      "val Loss: 0.2057 Acc: 0.9326\n",
      "\n",
      "Epoch 7/299\n",
      "----------\n",
      "train Loss: 0.2092 Acc: 0.9228\n",
      "val Loss: 0.2026 Acc: 0.9303\n",
      "\n",
      "Epoch 8/299\n",
      "----------\n",
      "train Loss: 0.2073 Acc: 0.9248\n",
      "val Loss: 0.2003 Acc: 0.9340\n",
      "\n",
      "Epoch 9/299\n",
      "----------\n",
      "train Loss: 0.2037 Acc: 0.9255\n",
      "val Loss: 0.1948 Acc: 0.9322\n",
      "\n",
      "Epoch 10/299\n",
      "----------\n",
      "train Loss: 0.2012 Acc: 0.9257\n",
      "val Loss: 0.1938 Acc: 0.9386\n",
      "\n",
      "Epoch 11/299\n",
      "----------\n",
      "train Loss: 0.2012 Acc: 0.9251\n",
      "val Loss: 0.1900 Acc: 0.9354\n",
      "\n",
      "Epoch 12/299\n",
      "----------\n",
      "train Loss: 0.2029 Acc: 0.9237\n",
      "val Loss: 0.1950 Acc: 0.9354\n",
      "\n",
      "Epoch 13/299\n",
      "----------\n",
      "train Loss: 0.1985 Acc: 0.9292\n",
      "val Loss: 0.1980 Acc: 0.9363\n",
      "\n",
      "Epoch 14/299\n",
      "----------\n",
      "train Loss: 0.1968 Acc: 0.9290\n",
      "val Loss: 0.1928 Acc: 0.9322\n",
      "\n",
      "Epoch 15/299\n",
      "----------\n",
      "train Loss: 0.1958 Acc: 0.9278\n",
      "val Loss: 0.1956 Acc: 0.9308\n",
      "\n",
      "Epoch 16/299\n",
      "----------\n",
      "train Loss: 0.1942 Acc: 0.9252\n",
      "val Loss: 0.1839 Acc: 0.9349\n",
      "\n",
      "Epoch 17/299\n",
      "----------\n",
      "train Loss: 0.1955 Acc: 0.9287\n",
      "val Loss: 0.1906 Acc: 0.9345\n",
      "\n",
      "Epoch 18/299\n",
      "----------\n",
      "train Loss: 0.1908 Acc: 0.9285\n",
      "val Loss: 0.1919 Acc: 0.9340\n",
      "\n",
      "Epoch 19/299\n",
      "----------\n",
      "train Loss: 0.1927 Acc: 0.9290\n",
      "val Loss: 0.1954 Acc: 0.9340\n",
      "\n",
      "Epoch 20/299\n",
      "----------\n",
      "train Loss: 0.1908 Acc: 0.9290\n",
      "val Loss: 0.1898 Acc: 0.9322\n",
      "\n",
      "Epoch 21/299\n",
      "----------\n",
      "train Loss: 0.1914 Acc: 0.9298\n",
      "val Loss: 0.1889 Acc: 0.9372\n",
      "\n",
      "Epoch 22/299\n",
      "----------\n",
      "train Loss: 0.1924 Acc: 0.9282\n",
      "val Loss: 0.1877 Acc: 0.9359\n",
      "\n",
      "Epoch 23/299\n",
      "----------\n",
      "train Loss: 0.1898 Acc: 0.9294\n",
      "val Loss: 0.1836 Acc: 0.9372\n",
      "\n",
      "Epoch 24/299\n",
      "----------\n",
      "train Loss: 0.1896 Acc: 0.9300\n",
      "val Loss: 0.1842 Acc: 0.9368\n",
      "\n",
      "Epoch 25/299\n",
      "----------\n",
      "train Loss: 0.1904 Acc: 0.9283\n",
      "val Loss: 0.1827 Acc: 0.9345\n",
      "\n",
      "Epoch 26/299\n",
      "----------\n",
      "train Loss: 0.1884 Acc: 0.9298\n",
      "val Loss: 0.1804 Acc: 0.9382\n",
      "\n",
      "Epoch 27/299\n",
      "----------\n",
      "train Loss: 0.1876 Acc: 0.9297\n",
      "val Loss: 0.1847 Acc: 0.9331\n",
      "\n",
      "Epoch 28/299\n",
      "----------\n",
      "train Loss: 0.1882 Acc: 0.9288\n",
      "val Loss: 0.1837 Acc: 0.9345\n",
      "\n",
      "Epoch 29/299\n",
      "----------\n",
      "train Loss: 0.1870 Acc: 0.9298\n",
      "val Loss: 0.1876 Acc: 0.9372\n",
      "\n",
      "Epoch 30/299\n",
      "----------\n",
      "train Loss: 0.1888 Acc: 0.9288\n",
      "val Loss: 0.1830 Acc: 0.9308\n",
      "\n",
      "Epoch 31/299\n",
      "----------\n",
      "train Loss: 0.1877 Acc: 0.9288\n",
      "val Loss: 0.1806 Acc: 0.9372\n",
      "\n",
      "Epoch 32/299\n",
      "----------\n",
      "train Loss: 0.1857 Acc: 0.9302\n",
      "val Loss: 0.1851 Acc: 0.9312\n",
      "\n",
      "Epoch 33/299\n",
      "----------\n",
      "train Loss: 0.1836 Acc: 0.9324\n",
      "val Loss: 0.1897 Acc: 0.9312\n",
      "\n",
      "Epoch 34/299\n",
      "----------\n",
      "train Loss: 0.1854 Acc: 0.9326\n",
      "val Loss: 0.1892 Acc: 0.9335\n",
      "\n",
      "Epoch 35/299\n",
      "----------\n",
      "train Loss: 0.1831 Acc: 0.9323\n",
      "val Loss: 0.1830 Acc: 0.9335\n",
      "\n",
      "Epoch 36/299\n",
      "----------\n",
      "train Loss: 0.1825 Acc: 0.9310\n",
      "val Loss: 0.1906 Acc: 0.9335\n",
      "\n",
      "Epoch 37/299\n",
      "----------\n",
      "train Loss: 0.1807 Acc: 0.9322\n",
      "val Loss: 0.1781 Acc: 0.9377\n",
      "\n",
      "Epoch 38/299\n",
      "----------\n",
      "train Loss: 0.1820 Acc: 0.9338\n",
      "val Loss: 0.1831 Acc: 0.9391\n",
      "\n",
      "Epoch 39/299\n",
      "----------\n",
      "train Loss: 0.1796 Acc: 0.9339\n",
      "val Loss: 0.1789 Acc: 0.9377\n",
      "\n",
      "Epoch 40/299\n",
      "----------\n",
      "train Loss: 0.1814 Acc: 0.9325\n",
      "val Loss: 0.1806 Acc: 0.9363\n",
      "\n",
      "Epoch 41/299\n",
      "----------\n",
      "train Loss: 0.1807 Acc: 0.9348\n",
      "val Loss: 0.1841 Acc: 0.9386\n",
      "\n",
      "Epoch 42/299\n",
      "----------\n",
      "train Loss: 0.1806 Acc: 0.9319\n",
      "val Loss: 0.2021 Acc: 0.9322\n",
      "\n",
      "Epoch 43/299\n",
      "----------\n",
      "train Loss: 0.1831 Acc: 0.9319\n",
      "val Loss: 0.1811 Acc: 0.9345\n",
      "\n",
      "Epoch 44/299\n",
      "----------\n",
      "train Loss: 0.1807 Acc: 0.9323\n",
      "val Loss: 0.1827 Acc: 0.9400\n",
      "\n",
      "Epoch 45/299\n",
      "----------\n",
      "train Loss: 0.1806 Acc: 0.9327\n",
      "val Loss: 0.1818 Acc: 0.9405\n",
      "\n",
      "Epoch 46/299\n",
      "----------\n",
      "train Loss: 0.1787 Acc: 0.9338\n",
      "val Loss: 0.1742 Acc: 0.9391\n",
      "\n",
      "Epoch 47/299\n",
      "----------\n",
      "train Loss: 0.1779 Acc: 0.9332\n",
      "val Loss: 0.1891 Acc: 0.9349\n",
      "\n",
      "Epoch 48/299\n",
      "----------\n",
      "train Loss: 0.1791 Acc: 0.9332\n",
      "val Loss: 0.1768 Acc: 0.9395\n",
      "\n",
      "Epoch 49/299\n",
      "----------\n",
      "train Loss: 0.1772 Acc: 0.9345\n",
      "val Loss: 0.1871 Acc: 0.9368\n",
      "\n",
      "Epoch 50/299\n",
      "----------\n",
      "train Loss: 0.1802 Acc: 0.9338\n",
      "val Loss: 0.1742 Acc: 0.9395\n",
      "\n",
      "Epoch 51/299\n",
      "----------\n",
      "train Loss: 0.1777 Acc: 0.9330\n",
      "val Loss: 0.1885 Acc: 0.9372\n",
      "\n",
      "Epoch 52/299\n",
      "----------\n",
      "train Loss: 0.1761 Acc: 0.9350\n",
      "val Loss: 0.1874 Acc: 0.9322\n",
      "\n",
      "Epoch 53/299\n",
      "----------\n",
      "train Loss: 0.1758 Acc: 0.9349\n",
      "val Loss: 0.2035 Acc: 0.9280\n",
      "\n",
      "Epoch 54/299\n",
      "----------\n",
      "train Loss: 0.1742 Acc: 0.9332\n",
      "val Loss: 0.1848 Acc: 0.9335\n",
      "\n",
      "Epoch 55/299\n",
      "----------\n",
      "train Loss: 0.1752 Acc: 0.9325\n",
      "val Loss: 0.1745 Acc: 0.9372\n",
      "\n",
      "Epoch 56/299\n",
      "----------\n",
      "train Loss: 0.1753 Acc: 0.9327\n",
      "val Loss: 0.1771 Acc: 0.9363\n",
      "\n",
      "Epoch 57/299\n",
      "----------\n",
      "train Loss: 0.1744 Acc: 0.9349\n",
      "val Loss: 0.1806 Acc: 0.9382\n",
      "\n",
      "Epoch 58/299\n",
      "----------\n",
      "train Loss: 0.1774 Acc: 0.9347\n",
      "val Loss: 0.1879 Acc: 0.9340\n",
      "\n",
      "Epoch 59/299\n",
      "----------\n",
      "train Loss: 0.1725 Acc: 0.9358\n",
      "val Loss: 0.1891 Acc: 0.9331\n",
      "\n",
      "Epoch 60/299\n",
      "----------\n",
      "train Loss: 0.1748 Acc: 0.9341\n",
      "val Loss: 0.1757 Acc: 0.9391\n",
      "\n",
      "Epoch 61/299\n",
      "----------\n",
      "train Loss: 0.1736 Acc: 0.9361\n",
      "val Loss: 0.1850 Acc: 0.9386\n",
      "\n",
      "Epoch 62/299\n",
      "----------\n",
      "train Loss: 0.1740 Acc: 0.9354\n",
      "val Loss: 0.1759 Acc: 0.9354\n",
      "\n",
      "Epoch 63/299\n",
      "----------\n",
      "train Loss: 0.1765 Acc: 0.9328\n",
      "val Loss: 0.1941 Acc: 0.9285\n",
      "\n",
      "Epoch 64/299\n",
      "----------\n",
      "train Loss: 0.1725 Acc: 0.9356\n",
      "val Loss: 0.1788 Acc: 0.9405\n",
      "\n",
      "Epoch 65/299\n",
      "----------\n",
      "train Loss: 0.1707 Acc: 0.9358\n",
      "val Loss: 0.1752 Acc: 0.9395\n",
      "\n",
      "Epoch 66/299\n",
      "----------\n",
      "train Loss: 0.1727 Acc: 0.9348\n",
      "val Loss: 0.1734 Acc: 0.9359\n",
      "\n",
      "Epoch 67/299\n",
      "----------\n",
      "train Loss: 0.1709 Acc: 0.9347\n",
      "val Loss: 0.1784 Acc: 0.9354\n",
      "\n",
      "Epoch 68/299\n",
      "----------\n",
      "train Loss: 0.1710 Acc: 0.9367\n",
      "val Loss: 0.1764 Acc: 0.9395\n",
      "\n",
      "Epoch 69/299\n",
      "----------\n",
      "train Loss: 0.1702 Acc: 0.9363\n",
      "val Loss: 0.1760 Acc: 0.9349\n",
      "\n",
      "Epoch 70/299\n",
      "----------\n",
      "train Loss: 0.1690 Acc: 0.9379\n",
      "val Loss: 0.1766 Acc: 0.9372\n",
      "\n",
      "Epoch 71/299\n",
      "----------\n",
      "train Loss: 0.1703 Acc: 0.9375\n",
      "val Loss: 0.1766 Acc: 0.9395\n",
      "\n",
      "Epoch 72/299\n",
      "----------\n",
      "train Loss: 0.1708 Acc: 0.9352\n",
      "val Loss: 0.1881 Acc: 0.9340\n",
      "\n",
      "Epoch 73/299\n",
      "----------\n",
      "train Loss: 0.1728 Acc: 0.9356\n",
      "val Loss: 0.1905 Acc: 0.9317\n",
      "\n",
      "Epoch 74/299\n",
      "----------\n",
      "train Loss: 0.1710 Acc: 0.9356\n",
      "val Loss: 0.1729 Acc: 0.9386\n",
      "\n",
      "Epoch 75/299\n",
      "----------\n",
      "train Loss: 0.1672 Acc: 0.9383\n",
      "val Loss: 0.1836 Acc: 0.9349\n",
      "\n",
      "Epoch 76/299\n",
      "----------\n",
      "train Loss: 0.1686 Acc: 0.9378\n",
      "val Loss: 0.1801 Acc: 0.9382\n",
      "\n",
      "Epoch 77/299\n",
      "----------\n",
      "train Loss: 0.1681 Acc: 0.9375\n",
      "val Loss: 0.1966 Acc: 0.9317\n",
      "\n",
      "Epoch 78/299\n",
      "----------\n",
      "train Loss: 0.1717 Acc: 0.9361\n",
      "val Loss: 0.1867 Acc: 0.9359\n",
      "\n",
      "Epoch 79/299\n",
      "----------\n",
      "train Loss: 0.1692 Acc: 0.9372\n",
      "val Loss: 0.1724 Acc: 0.9400\n",
      "\n",
      "Epoch 80/299\n",
      "----------\n",
      "train Loss: 0.1704 Acc: 0.9350\n",
      "val Loss: 0.1869 Acc: 0.9312\n",
      "\n",
      "Epoch 81/299\n",
      "----------\n",
      "train Loss: 0.1679 Acc: 0.9356\n",
      "val Loss: 0.1783 Acc: 0.9354\n",
      "\n",
      "Epoch 82/299\n",
      "----------\n",
      "train Loss: 0.1686 Acc: 0.9367\n",
      "val Loss: 0.1861 Acc: 0.9331\n",
      "\n",
      "Epoch 83/299\n",
      "----------\n",
      "train Loss: 0.1660 Acc: 0.9365\n",
      "val Loss: 0.1806 Acc: 0.9345\n",
      "\n",
      "Epoch 84/299\n",
      "----------\n",
      "train Loss: 0.1680 Acc: 0.9357\n",
      "val Loss: 0.1896 Acc: 0.9354\n",
      "\n",
      "Epoch 85/299\n",
      "----------\n",
      "train Loss: 0.1699 Acc: 0.9356\n",
      "val Loss: 0.1728 Acc: 0.9377\n",
      "\n",
      "Epoch 86/299\n",
      "----------\n",
      "train Loss: 0.1665 Acc: 0.9358\n",
      "val Loss: 0.1799 Acc: 0.9363\n",
      "\n",
      "Epoch 87/299\n",
      "----------\n",
      "train Loss: 0.1657 Acc: 0.9348\n",
      "val Loss: 0.1734 Acc: 0.9377\n",
      "\n",
      "Epoch 88/299\n",
      "----------\n",
      "train Loss: 0.1670 Acc: 0.9379\n",
      "val Loss: 0.1750 Acc: 0.9359\n",
      "\n",
      "Epoch 89/299\n",
      "----------\n",
      "train Loss: 0.1656 Acc: 0.9384\n",
      "val Loss: 0.1763 Acc: 0.9340\n",
      "\n",
      "Epoch 90/299\n",
      "----------\n",
      "train Loss: 0.1648 Acc: 0.9407\n",
      "val Loss: 0.1827 Acc: 0.9377\n",
      "\n",
      "Epoch 91/299\n",
      "----------\n",
      "train Loss: 0.1649 Acc: 0.9379\n",
      "val Loss: 0.1885 Acc: 0.9349\n",
      "\n",
      "Epoch 92/299\n",
      "----------\n",
      "train Loss: 0.1653 Acc: 0.9375\n",
      "val Loss: 0.1796 Acc: 0.9345\n",
      "\n",
      "Epoch 93/299\n",
      "----------\n",
      "train Loss: 0.1645 Acc: 0.9384\n",
      "val Loss: 0.1855 Acc: 0.9340\n",
      "\n",
      "Epoch 94/299\n",
      "----------\n",
      "train Loss: 0.1660 Acc: 0.9363\n",
      "val Loss: 0.1833 Acc: 0.9377\n",
      "\n",
      "Epoch 95/299\n",
      "----------\n",
      "train Loss: 0.1640 Acc: 0.9371\n",
      "val Loss: 0.1745 Acc: 0.9414\n",
      "\n",
      "Epoch 96/299\n",
      "----------\n",
      "train Loss: 0.1648 Acc: 0.9376\n",
      "val Loss: 0.1797 Acc: 0.9386\n",
      "\n",
      "Epoch 97/299\n",
      "----------\n",
      "train Loss: 0.1623 Acc: 0.9377\n",
      "val Loss: 0.1775 Acc: 0.9349\n",
      "\n",
      "Epoch 98/299\n",
      "----------\n",
      "train Loss: 0.1637 Acc: 0.9399\n",
      "val Loss: 0.1745 Acc: 0.9363\n",
      "\n",
      "Epoch 99/299\n",
      "----------\n",
      "train Loss: 0.1627 Acc: 0.9379\n",
      "val Loss: 0.1793 Acc: 0.9372\n",
      "\n",
      "Epoch 100/299\n",
      "----------\n",
      "train Loss: 0.1620 Acc: 0.9353\n",
      "val Loss: 0.1729 Acc: 0.9368\n",
      "\n",
      "Epoch 101/299\n",
      "----------\n",
      "train Loss: 0.1628 Acc: 0.9368\n",
      "val Loss: 0.1799 Acc: 0.9359\n",
      "\n",
      "Epoch 102/299\n",
      "----------\n",
      "train Loss: 0.1656 Acc: 0.9392\n",
      "val Loss: 0.1850 Acc: 0.9349\n",
      "\n",
      "Epoch 103/299\n",
      "----------\n",
      "train Loss: 0.1618 Acc: 0.9387\n",
      "val Loss: 0.2011 Acc: 0.9252\n",
      "\n",
      "Epoch 104/299\n",
      "----------\n",
      "train Loss: 0.1636 Acc: 0.9372\n",
      "val Loss: 0.1739 Acc: 0.9400\n",
      "\n",
      "Epoch 105/299\n",
      "----------\n",
      "train Loss: 0.1625 Acc: 0.9372\n",
      "val Loss: 0.1842 Acc: 0.9335\n",
      "\n",
      "Epoch 106/299\n",
      "----------\n",
      "train Loss: 0.1621 Acc: 0.9388\n",
      "val Loss: 0.1797 Acc: 0.9391\n",
      "\n",
      "Epoch 107/299\n",
      "----------\n",
      "train Loss: 0.1625 Acc: 0.9364\n",
      "val Loss: 0.1843 Acc: 0.9345\n",
      "\n",
      "Epoch 108/299\n",
      "----------\n",
      "train Loss: 0.1617 Acc: 0.9388\n",
      "val Loss: 0.1791 Acc: 0.9400\n",
      "\n",
      "Epoch 109/299\n",
      "----------\n",
      "train Loss: 0.1627 Acc: 0.9384\n",
      "val Loss: 0.1861 Acc: 0.9354\n",
      "\n",
      "Epoch 110/299\n",
      "----------\n",
      "train Loss: 0.1580 Acc: 0.9408\n",
      "val Loss: 0.1757 Acc: 0.9368\n",
      "\n",
      "Epoch 111/299\n",
      "----------\n",
      "train Loss: 0.1594 Acc: 0.9394\n",
      "val Loss: 0.1821 Acc: 0.9386\n",
      "\n",
      "Epoch 112/299\n",
      "----------\n",
      "train Loss: 0.1587 Acc: 0.9399\n",
      "val Loss: 0.1850 Acc: 0.9317\n",
      "\n",
      "Epoch 113/299\n",
      "----------\n",
      "train Loss: 0.1600 Acc: 0.9401\n",
      "val Loss: 0.1776 Acc: 0.9391\n",
      "\n",
      "Epoch 114/299\n",
      "----------\n",
      "train Loss: 0.1583 Acc: 0.9408\n",
      "val Loss: 0.1782 Acc: 0.9377\n",
      "\n",
      "Epoch 115/299\n",
      "----------\n",
      "train Loss: 0.1610 Acc: 0.9390\n",
      "val Loss: 0.1788 Acc: 0.9372\n",
      "\n",
      "Epoch 116/299\n",
      "----------\n",
      "train Loss: 0.1586 Acc: 0.9383\n",
      "val Loss: 0.1918 Acc: 0.9340\n",
      "\n",
      "Epoch 117/299\n",
      "----------\n",
      "train Loss: 0.1582 Acc: 0.9388\n",
      "val Loss: 0.1830 Acc: 0.9363\n",
      "\n",
      "Epoch 118/299\n",
      "----------\n",
      "train Loss: 0.1593 Acc: 0.9373\n",
      "val Loss: 0.1898 Acc: 0.9335\n",
      "\n",
      "Epoch 119/299\n",
      "----------\n",
      "train Loss: 0.1608 Acc: 0.9388\n",
      "val Loss: 0.1771 Acc: 0.9377\n",
      "\n",
      "Epoch 120/299\n",
      "----------\n",
      "train Loss: 0.1577 Acc: 0.9395\n",
      "val Loss: 0.2009 Acc: 0.9266\n",
      "\n",
      "Epoch 121/299\n",
      "----------\n",
      "train Loss: 0.1595 Acc: 0.9375\n",
      "val Loss: 0.1749 Acc: 0.9363\n",
      "\n",
      "Epoch 122/299\n",
      "----------\n",
      "train Loss: 0.1561 Acc: 0.9394\n",
      "val Loss: 0.1787 Acc: 0.9377\n",
      "\n",
      "Epoch 123/299\n",
      "----------\n",
      "train Loss: 0.1578 Acc: 0.9392\n",
      "val Loss: 0.1802 Acc: 0.9372\n",
      "\n",
      "Epoch 124/299\n",
      "----------\n",
      "train Loss: 0.1555 Acc: 0.9410\n",
      "val Loss: 0.1763 Acc: 0.9354\n",
      "\n",
      "Epoch 125/299\n",
      "----------\n",
      "train Loss: 0.1584 Acc: 0.9401\n",
      "val Loss: 0.1847 Acc: 0.9377\n",
      "\n",
      "Epoch 126/299\n",
      "----------\n",
      "train Loss: 0.1601 Acc: 0.9371\n",
      "val Loss: 0.1845 Acc: 0.9345\n",
      "\n",
      "Epoch 127/299\n",
      "----------\n",
      "train Loss: 0.1571 Acc: 0.9401\n",
      "val Loss: 0.1921 Acc: 0.9335\n",
      "\n",
      "Epoch 128/299\n",
      "----------\n",
      "train Loss: 0.1577 Acc: 0.9393\n",
      "val Loss: 0.1936 Acc: 0.9340\n",
      "\n",
      "Epoch 129/299\n",
      "----------\n",
      "train Loss: 0.1587 Acc: 0.9392\n",
      "val Loss: 0.1800 Acc: 0.9340\n",
      "\n",
      "Epoch 130/299\n",
      "----------\n",
      "train Loss: 0.1567 Acc: 0.9398\n",
      "val Loss: 0.1765 Acc: 0.9359\n",
      "\n",
      "Epoch 131/299\n",
      "----------\n",
      "train Loss: 0.1535 Acc: 0.9387\n",
      "val Loss: 0.1780 Acc: 0.9409\n",
      "\n",
      "Epoch 132/299\n",
      "----------\n",
      "train Loss: 0.1564 Acc: 0.9395\n",
      "val Loss: 0.1812 Acc: 0.9359\n",
      "\n",
      "Epoch 133/299\n",
      "----------\n",
      "train Loss: 0.1553 Acc: 0.9392\n",
      "val Loss: 0.1902 Acc: 0.9340\n",
      "\n",
      "Epoch 134/299\n",
      "----------\n",
      "train Loss: 0.1530 Acc: 0.9407\n",
      "val Loss: 0.1861 Acc: 0.9331\n",
      "\n",
      "Epoch 135/299\n",
      "----------\n",
      "train Loss: 0.1578 Acc: 0.9391\n",
      "val Loss: 0.1814 Acc: 0.9331\n",
      "\n",
      "Epoch 136/299\n",
      "----------\n",
      "train Loss: 0.1555 Acc: 0.9403\n",
      "val Loss: 0.1873 Acc: 0.9349\n",
      "\n",
      "Epoch 137/299\n",
      "----------\n",
      "train Loss: 0.1522 Acc: 0.9391\n",
      "val Loss: 0.1880 Acc: 0.9359\n",
      "\n",
      "Epoch 138/299\n",
      "----------\n",
      "train Loss: 0.1578 Acc: 0.9391\n",
      "val Loss: 0.1775 Acc: 0.9382\n",
      "\n",
      "Epoch 139/299\n",
      "----------\n",
      "train Loss: 0.1540 Acc: 0.9386\n",
      "val Loss: 0.1837 Acc: 0.9377\n",
      "\n",
      "Epoch 140/299\n",
      "----------\n",
      "train Loss: 0.1526 Acc: 0.9406\n",
      "val Loss: 0.1814 Acc: 0.9359\n",
      "\n",
      "Epoch 141/299\n",
      "----------\n",
      "train Loss: 0.1517 Acc: 0.9414\n",
      "val Loss: 0.1876 Acc: 0.9349\n",
      "\n",
      "Epoch 142/299\n",
      "----------\n",
      "train Loss: 0.1555 Acc: 0.9393\n",
      "val Loss: 0.2062 Acc: 0.9285\n",
      "\n",
      "Epoch 143/299\n",
      "----------\n",
      "train Loss: 0.1520 Acc: 0.9403\n",
      "val Loss: 0.1870 Acc: 0.9372\n",
      "\n",
      "Epoch 144/299\n",
      "----------\n",
      "train Loss: 0.1546 Acc: 0.9415\n",
      "val Loss: 0.1846 Acc: 0.9363\n",
      "\n",
      "Epoch 145/299\n",
      "----------\n",
      "train Loss: 0.1569 Acc: 0.9405\n",
      "val Loss: 0.1849 Acc: 0.9354\n",
      "\n",
      "Epoch 146/299\n",
      "----------\n",
      "train Loss: 0.1544 Acc: 0.9409\n",
      "val Loss: 0.1787 Acc: 0.9372\n",
      "\n",
      "Epoch 147/299\n",
      "----------\n",
      "train Loss: 0.1494 Acc: 0.9412\n",
      "val Loss: 0.1819 Acc: 0.9345\n",
      "\n",
      "Epoch 148/299\n",
      "----------\n",
      "train Loss: 0.1552 Acc: 0.9384\n",
      "val Loss: 0.1843 Acc: 0.9368\n",
      "\n",
      "Epoch 149/299\n",
      "----------\n",
      "train Loss: 0.1513 Acc: 0.9424\n",
      "val Loss: 0.1777 Acc: 0.9391\n",
      "\n",
      "Epoch 150/299\n",
      "----------\n",
      "train Loss: 0.1511 Acc: 0.9408\n",
      "val Loss: 0.1940 Acc: 0.9345\n",
      "\n",
      "Epoch 151/299\n",
      "----------\n",
      "train Loss: 0.1513 Acc: 0.9405\n",
      "val Loss: 0.1825 Acc: 0.9354\n",
      "\n",
      "Epoch 152/299\n",
      "----------\n",
      "train Loss: 0.1535 Acc: 0.9402\n",
      "val Loss: 0.1843 Acc: 0.9372\n",
      "\n",
      "Epoch 153/299\n",
      "----------\n",
      "train Loss: 0.1526 Acc: 0.9420\n",
      "val Loss: 0.2092 Acc: 0.9299\n",
      "\n",
      "Epoch 154/299\n",
      "----------\n",
      "train Loss: 0.1516 Acc: 0.9403\n",
      "val Loss: 0.1912 Acc: 0.9322\n",
      "\n",
      "Epoch 155/299\n",
      "----------\n",
      "train Loss: 0.1521 Acc: 0.9410\n",
      "val Loss: 0.1853 Acc: 0.9359\n",
      "\n",
      "Epoch 156/299\n",
      "----------\n",
      "train Loss: 0.1518 Acc: 0.9405\n",
      "val Loss: 0.1747 Acc: 0.9377\n",
      "\n",
      "Epoch 157/299\n",
      "----------\n",
      "train Loss: 0.1497 Acc: 0.9412\n",
      "val Loss: 0.1847 Acc: 0.9345\n",
      "\n",
      "Epoch 158/299\n",
      "----------\n",
      "train Loss: 0.1482 Acc: 0.9422\n",
      "val Loss: 0.1887 Acc: 0.9335\n",
      "\n",
      "Epoch 159/299\n",
      "----------\n",
      "train Loss: 0.1491 Acc: 0.9413\n",
      "val Loss: 0.1897 Acc: 0.9395\n",
      "\n",
      "Epoch 160/299\n",
      "----------\n",
      "train Loss: 0.1494 Acc: 0.9418\n",
      "val Loss: 0.1810 Acc: 0.9354\n",
      "\n",
      "Epoch 161/299\n",
      "----------\n",
      "train Loss: 0.1503 Acc: 0.9430\n",
      "val Loss: 0.1790 Acc: 0.9386\n",
      "\n",
      "Epoch 162/299\n",
      "----------\n",
      "train Loss: 0.1484 Acc: 0.9416\n",
      "val Loss: 0.1935 Acc: 0.9368\n",
      "\n",
      "Epoch 163/299\n",
      "----------\n",
      "train Loss: 0.1558 Acc: 0.9393\n",
      "val Loss: 0.1904 Acc: 0.9340\n",
      "\n",
      "Epoch 164/299\n",
      "----------\n",
      "train Loss: 0.1522 Acc: 0.9400\n",
      "val Loss: 0.1863 Acc: 0.9345\n",
      "\n",
      "Epoch 165/299\n",
      "----------\n",
      "train Loss: 0.1499 Acc: 0.9427\n",
      "val Loss: 0.1828 Acc: 0.9359\n",
      "\n",
      "Epoch 166/299\n",
      "----------\n",
      "train Loss: 0.1471 Acc: 0.9417\n",
      "val Loss: 0.1817 Acc: 0.9359\n",
      "\n",
      "Epoch 167/299\n",
      "----------\n",
      "train Loss: 0.1521 Acc: 0.9414\n",
      "val Loss: 0.1865 Acc: 0.9349\n",
      "\n",
      "Epoch 168/299\n",
      "----------\n",
      "train Loss: 0.1500 Acc: 0.9386\n",
      "val Loss: 0.1862 Acc: 0.9363\n",
      "\n",
      "Epoch 169/299\n",
      "----------\n",
      "train Loss: 0.1460 Acc: 0.9430\n",
      "val Loss: 0.1949 Acc: 0.9345\n",
      "\n",
      "Epoch 170/299\n",
      "----------\n",
      "train Loss: 0.1476 Acc: 0.9409\n",
      "val Loss: 0.1835 Acc: 0.9377\n",
      "\n",
      "Epoch 171/299\n",
      "----------\n",
      "train Loss: 0.1529 Acc: 0.9373\n",
      "val Loss: 0.1936 Acc: 0.9331\n",
      "\n",
      "Epoch 172/299\n",
      "----------\n",
      "train Loss: 0.1475 Acc: 0.9439\n",
      "val Loss: 0.1897 Acc: 0.9331\n",
      "\n",
      "Epoch 173/299\n",
      "----------\n",
      "train Loss: 0.1445 Acc: 0.9428\n",
      "val Loss: 0.1903 Acc: 0.9331\n",
      "\n",
      "Epoch 174/299\n",
      "----------\n",
      "train Loss: 0.1468 Acc: 0.9423\n",
      "val Loss: 0.1872 Acc: 0.9359\n",
      "\n",
      "Epoch 175/299\n",
      "----------\n",
      "train Loss: 0.1474 Acc: 0.9400\n",
      "val Loss: 0.1962 Acc: 0.9345\n",
      "\n",
      "Epoch 176/299\n",
      "----------\n",
      "train Loss: 0.1461 Acc: 0.9435\n",
      "val Loss: 0.1907 Acc: 0.9354\n",
      "\n",
      "Epoch 177/299\n",
      "----------\n",
      "train Loss: 0.1462 Acc: 0.9429\n",
      "val Loss: 0.1978 Acc: 0.9322\n",
      "\n",
      "Epoch 178/299\n",
      "----------\n",
      "train Loss: 0.1436 Acc: 0.9445\n",
      "val Loss: 0.1826 Acc: 0.9359\n",
      "\n",
      "Epoch 179/299\n",
      "----------\n",
      "train Loss: 0.1480 Acc: 0.9412\n",
      "val Loss: 0.1968 Acc: 0.9331\n",
      "\n",
      "Epoch 180/299\n",
      "----------\n",
      "train Loss: 0.1478 Acc: 0.9415\n",
      "val Loss: 0.1951 Acc: 0.9275\n",
      "\n",
      "Epoch 181/299\n",
      "----------\n",
      "train Loss: 0.1478 Acc: 0.9413\n",
      "val Loss: 0.1879 Acc: 0.9368\n",
      "\n",
      "Epoch 182/299\n",
      "----------\n",
      "train Loss: 0.1459 Acc: 0.9409\n",
      "val Loss: 0.1817 Acc: 0.9400\n",
      "\n",
      "Epoch 183/299\n",
      "----------\n",
      "train Loss: 0.1473 Acc: 0.9428\n",
      "val Loss: 0.1861 Acc: 0.9372\n",
      "\n",
      "Epoch 184/299\n",
      "----------\n",
      "train Loss: 0.1438 Acc: 0.9443\n",
      "val Loss: 0.1843 Acc: 0.9354\n",
      "\n",
      "Epoch 185/299\n",
      "----------\n",
      "train Loss: 0.1443 Acc: 0.9451\n",
      "val Loss: 0.1862 Acc: 0.9368\n",
      "\n",
      "Epoch 186/299\n",
      "----------\n",
      "train Loss: 0.1454 Acc: 0.9442\n",
      "val Loss: 0.1850 Acc: 0.9377\n",
      "\n",
      "Epoch 187/299\n",
      "----------\n",
      "train Loss: 0.1436 Acc: 0.9442\n",
      "val Loss: 0.1915 Acc: 0.9359\n",
      "\n",
      "Epoch 188/299\n",
      "----------\n",
      "train Loss: 0.1452 Acc: 0.9444\n",
      "val Loss: 0.1894 Acc: 0.9368\n",
      "\n",
      "Epoch 189/299\n",
      "----------\n",
      "train Loss: 0.1438 Acc: 0.9429\n",
      "val Loss: 0.1838 Acc: 0.9363\n",
      "\n",
      "Epoch 190/299\n",
      "----------\n",
      "train Loss: 0.1442 Acc: 0.9430\n",
      "val Loss: 0.1810 Acc: 0.9377\n",
      "\n",
      "Epoch 191/299\n",
      "----------\n",
      "train Loss: 0.1449 Acc: 0.9430\n",
      "val Loss: 0.1825 Acc: 0.9354\n",
      "\n",
      "Epoch 192/299\n",
      "----------\n",
      "train Loss: 0.1408 Acc: 0.9427\n",
      "val Loss: 0.1838 Acc: 0.9368\n",
      "\n",
      "Epoch 193/299\n",
      "----------\n",
      "train Loss: 0.1445 Acc: 0.9428\n",
      "val Loss: 0.1940 Acc: 0.9354\n",
      "\n",
      "Epoch 194/299\n",
      "----------\n",
      "train Loss: 0.1429 Acc: 0.9455\n",
      "val Loss: 0.1862 Acc: 0.9382\n",
      "\n",
      "Epoch 195/299\n",
      "----------\n",
      "train Loss: 0.1413 Acc: 0.9450\n",
      "val Loss: 0.1776 Acc: 0.9405\n",
      "\n",
      "Epoch 196/299\n",
      "----------\n",
      "train Loss: 0.1427 Acc: 0.9440\n",
      "val Loss: 0.1887 Acc: 0.9391\n",
      "\n",
      "Epoch 197/299\n",
      "----------\n",
      "train Loss: 0.1432 Acc: 0.9431\n",
      "val Loss: 0.2054 Acc: 0.9308\n",
      "\n",
      "Epoch 198/299\n",
      "----------\n",
      "train Loss: 0.1429 Acc: 0.9450\n",
      "val Loss: 0.1978 Acc: 0.9363\n",
      "\n",
      "Epoch 199/299\n",
      "----------\n",
      "train Loss: 0.1432 Acc: 0.9427\n",
      "val Loss: 0.1946 Acc: 0.9340\n",
      "\n",
      "Epoch 200/299\n",
      "----------\n",
      "train Loss: 0.1433 Acc: 0.9433\n",
      "val Loss: 0.1887 Acc: 0.9386\n",
      "\n",
      "Epoch 201/299\n",
      "----------\n",
      "train Loss: 0.1431 Acc: 0.9428\n",
      "val Loss: 0.1818 Acc: 0.9359\n",
      "\n",
      "Epoch 202/299\n",
      "----------\n",
      "train Loss: 0.1457 Acc: 0.9407\n",
      "val Loss: 0.1921 Acc: 0.9395\n",
      "\n",
      "Epoch 203/299\n",
      "----------\n",
      "train Loss: 0.1414 Acc: 0.9461\n",
      "val Loss: 0.1903 Acc: 0.9340\n",
      "\n",
      "Epoch 204/299\n",
      "----------\n",
      "train Loss: 0.1390 Acc: 0.9444\n",
      "val Loss: 0.1936 Acc: 0.9386\n",
      "\n",
      "Epoch 205/299\n",
      "----------\n",
      "train Loss: 0.1438 Acc: 0.9435\n",
      "val Loss: 0.1942 Acc: 0.9335\n",
      "\n",
      "Epoch 206/299\n",
      "----------\n",
      "train Loss: 0.1416 Acc: 0.9446\n",
      "val Loss: 0.1979 Acc: 0.9340\n",
      "\n",
      "Epoch 207/299\n",
      "----------\n",
      "train Loss: 0.1422 Acc: 0.9444\n",
      "val Loss: 0.2028 Acc: 0.9386\n",
      "\n",
      "Epoch 208/299\n",
      "----------\n",
      "train Loss: 0.1442 Acc: 0.9422\n",
      "val Loss: 0.1954 Acc: 0.9340\n",
      "\n",
      "Epoch 209/299\n",
      "----------\n",
      "train Loss: 0.1410 Acc: 0.9448\n",
      "val Loss: 0.1888 Acc: 0.9345\n",
      "\n",
      "Epoch 210/299\n",
      "----------\n",
      "train Loss: 0.1429 Acc: 0.9447\n",
      "val Loss: 0.1920 Acc: 0.9359\n",
      "\n",
      "Epoch 211/299\n",
      "----------\n",
      "train Loss: 0.1407 Acc: 0.9447\n",
      "val Loss: 0.1970 Acc: 0.9359\n",
      "\n",
      "Epoch 212/299\n",
      "----------\n",
      "train Loss: 0.1383 Acc: 0.9455\n",
      "val Loss: 0.1855 Acc: 0.9391\n",
      "\n",
      "Epoch 213/299\n",
      "----------\n",
      "train Loss: 0.1406 Acc: 0.9448\n",
      "val Loss: 0.1929 Acc: 0.9340\n",
      "\n",
      "Epoch 214/299\n",
      "----------\n",
      "train Loss: 0.1391 Acc: 0.9455\n",
      "val Loss: 0.2013 Acc: 0.9322\n",
      "\n",
      "Epoch 215/299\n",
      "----------\n",
      "train Loss: 0.1382 Acc: 0.9455\n",
      "val Loss: 0.2096 Acc: 0.9312\n",
      "\n",
      "Epoch 216/299\n",
      "----------\n",
      "train Loss: 0.1421 Acc: 0.9446\n",
      "val Loss: 0.1956 Acc: 0.9335\n",
      "\n",
      "Epoch 217/299\n",
      "----------\n",
      "train Loss: 0.1410 Acc: 0.9454\n",
      "val Loss: 0.1928 Acc: 0.9359\n",
      "\n",
      "Epoch 218/299\n",
      "----------\n",
      "train Loss: 0.1409 Acc: 0.9457\n",
      "val Loss: 0.2038 Acc: 0.9322\n",
      "\n",
      "Epoch 219/299\n",
      "----------\n",
      "train Loss: 0.1435 Acc: 0.9444\n",
      "val Loss: 0.2359 Acc: 0.9239\n",
      "\n",
      "Epoch 220/299\n",
      "----------\n",
      "train Loss: 0.1417 Acc: 0.9450\n",
      "val Loss: 0.1910 Acc: 0.9363\n",
      "\n",
      "Epoch 221/299\n",
      "----------\n",
      "train Loss: 0.1416 Acc: 0.9435\n",
      "val Loss: 0.1905 Acc: 0.9372\n",
      "\n",
      "Epoch 222/299\n",
      "----------\n",
      "train Loss: 0.1410 Acc: 0.9468\n",
      "val Loss: 0.1948 Acc: 0.9386\n",
      "\n",
      "Epoch 223/299\n",
      "----------\n",
      "train Loss: 0.1377 Acc: 0.9429\n",
      "val Loss: 0.1834 Acc: 0.9382\n",
      "\n",
      "Epoch 224/299\n",
      "----------\n",
      "train Loss: 0.1350 Acc: 0.9476\n",
      "val Loss: 0.1887 Acc: 0.9395\n",
      "\n",
      "Epoch 225/299\n",
      "----------\n",
      "train Loss: 0.1332 Acc: 0.9468\n",
      "val Loss: 0.1921 Acc: 0.9391\n",
      "\n",
      "Epoch 226/299\n",
      "----------\n",
      "train Loss: 0.1359 Acc: 0.9472\n",
      "val Loss: 0.2012 Acc: 0.9322\n",
      "\n",
      "Epoch 227/299\n",
      "----------\n",
      "train Loss: 0.1388 Acc: 0.9442\n",
      "val Loss: 0.2165 Acc: 0.9303\n",
      "\n",
      "Epoch 228/299\n",
      "----------\n",
      "train Loss: 0.1387 Acc: 0.9438\n",
      "val Loss: 0.2026 Acc: 0.9312\n",
      "\n",
      "Epoch 229/299\n",
      "----------\n",
      "train Loss: 0.1359 Acc: 0.9462\n",
      "val Loss: 0.1936 Acc: 0.9326\n",
      "\n",
      "Epoch 230/299\n",
      "----------\n",
      "train Loss: 0.1336 Acc: 0.9470\n",
      "val Loss: 0.1928 Acc: 0.9331\n",
      "\n",
      "Epoch 231/299\n",
      "----------\n",
      "train Loss: 0.1397 Acc: 0.9458\n",
      "val Loss: 0.1982 Acc: 0.9331\n",
      "\n",
      "Epoch 232/299\n",
      "----------\n",
      "train Loss: 0.1352 Acc: 0.9487\n",
      "val Loss: 0.2019 Acc: 0.9331\n",
      "\n",
      "Epoch 233/299\n",
      "----------\n",
      "train Loss: 0.1344 Acc: 0.9474\n",
      "val Loss: 0.1965 Acc: 0.9382\n",
      "\n",
      "Epoch 234/299\n",
      "----------\n",
      "train Loss: 0.1335 Acc: 0.9484\n",
      "val Loss: 0.2005 Acc: 0.9243\n",
      "\n",
      "Epoch 235/299\n",
      "----------\n",
      "train Loss: 0.1349 Acc: 0.9487\n",
      "val Loss: 0.2012 Acc: 0.9322\n",
      "\n",
      "Epoch 236/299\n",
      "----------\n",
      "train Loss: 0.1361 Acc: 0.9455\n",
      "val Loss: 0.2188 Acc: 0.9345\n",
      "\n",
      "Epoch 237/299\n",
      "----------\n",
      "train Loss: 0.1334 Acc: 0.9473\n",
      "val Loss: 0.2077 Acc: 0.9257\n",
      "\n",
      "Epoch 238/299\n",
      "----------\n",
      "train Loss: 0.1362 Acc: 0.9448\n",
      "val Loss: 0.2002 Acc: 0.9349\n",
      "\n",
      "Epoch 239/299\n",
      "----------\n",
      "train Loss: 0.1334 Acc: 0.9480\n",
      "val Loss: 0.1941 Acc: 0.9368\n",
      "\n",
      "Epoch 240/299\n",
      "----------\n",
      "train Loss: 0.1355 Acc: 0.9478\n",
      "val Loss: 0.1877 Acc: 0.9335\n",
      "\n",
      "Epoch 241/299\n",
      "----------\n",
      "train Loss: 0.1374 Acc: 0.9478\n",
      "val Loss: 0.2052 Acc: 0.9363\n",
      "\n",
      "Epoch 242/299\n",
      "----------\n",
      "train Loss: 0.1348 Acc: 0.9477\n",
      "val Loss: 0.2144 Acc: 0.9275\n",
      "\n",
      "Epoch 243/299\n",
      "----------\n",
      "train Loss: 0.1314 Acc: 0.9477\n",
      "val Loss: 0.1944 Acc: 0.9368\n",
      "\n",
      "Epoch 244/299\n",
      "----------\n",
      "train Loss: 0.1329 Acc: 0.9503\n",
      "val Loss: 0.1916 Acc: 0.9395\n",
      "\n",
      "Epoch 245/299\n",
      "----------\n",
      "train Loss: 0.1337 Acc: 0.9467\n",
      "val Loss: 0.2010 Acc: 0.9359\n",
      "\n",
      "Epoch 246/299\n",
      "----------\n",
      "train Loss: 0.1342 Acc: 0.9477\n",
      "val Loss: 0.1988 Acc: 0.9368\n",
      "\n",
      "Epoch 247/299\n",
      "----------\n",
      "train Loss: 0.1360 Acc: 0.9463\n",
      "val Loss: 0.2153 Acc: 0.9335\n",
      "\n",
      "Epoch 248/299\n",
      "----------\n",
      "train Loss: 0.1442 Acc: 0.9439\n",
      "val Loss: 0.2283 Acc: 0.9252\n",
      "\n",
      "Epoch 249/299\n",
      "----------\n",
      "train Loss: 0.1392 Acc: 0.9459\n",
      "val Loss: 0.1899 Acc: 0.9359\n",
      "\n",
      "Epoch 250/299\n",
      "----------\n",
      "train Loss: 0.1327 Acc: 0.9465\n",
      "val Loss: 0.1963 Acc: 0.9363\n",
      "\n",
      "Epoch 251/299\n",
      "----------\n",
      "train Loss: 0.1316 Acc: 0.9487\n",
      "val Loss: 0.1935 Acc: 0.9349\n",
      "\n",
      "Epoch 252/299\n",
      "----------\n",
      "train Loss: 0.1316 Acc: 0.9481\n",
      "val Loss: 0.1971 Acc: 0.9377\n",
      "\n",
      "Epoch 253/299\n",
      "----------\n",
      "train Loss: 0.1336 Acc: 0.9475\n",
      "val Loss: 0.1975 Acc: 0.9368\n",
      "\n",
      "Epoch 254/299\n",
      "----------\n",
      "train Loss: 0.1292 Acc: 0.9505\n",
      "val Loss: 0.1933 Acc: 0.9377\n",
      "\n",
      "Epoch 255/299\n",
      "----------\n",
      "train Loss: 0.1316 Acc: 0.9470\n",
      "val Loss: 0.2066 Acc: 0.9312\n",
      "\n",
      "Epoch 256/299\n",
      "----------\n",
      "train Loss: 0.1326 Acc: 0.9455\n",
      "val Loss: 0.2001 Acc: 0.9359\n",
      "\n",
      "Epoch 257/299\n",
      "----------\n",
      "train Loss: 0.1306 Acc: 0.9507\n",
      "val Loss: 0.2118 Acc: 0.9345\n",
      "\n",
      "Epoch 258/299\n",
      "----------\n",
      "train Loss: 0.1383 Acc: 0.9455\n",
      "val Loss: 0.1990 Acc: 0.9345\n",
      "\n",
      "Epoch 259/299\n",
      "----------\n",
      "train Loss: 0.1339 Acc: 0.9469\n",
      "val Loss: 0.2114 Acc: 0.9322\n",
      "\n",
      "Epoch 260/299\n",
      "----------\n",
      "train Loss: 0.1316 Acc: 0.9492\n",
      "val Loss: 0.2144 Acc: 0.9294\n",
      "\n",
      "Epoch 261/299\n",
      "----------\n",
      "train Loss: 0.1336 Acc: 0.9474\n",
      "val Loss: 0.2036 Acc: 0.9303\n",
      "\n",
      "Epoch 262/299\n",
      "----------\n",
      "train Loss: 0.1351 Acc: 0.9460\n",
      "val Loss: 0.2170 Acc: 0.9317\n",
      "\n",
      "Epoch 263/299\n",
      "----------\n",
      "train Loss: 0.1290 Acc: 0.9504\n",
      "val Loss: 0.2105 Acc: 0.9248\n",
      "\n",
      "Epoch 264/299\n",
      "----------\n",
      "train Loss: 0.1321 Acc: 0.9489\n",
      "val Loss: 0.1983 Acc: 0.9372\n",
      "\n",
      "Epoch 265/299\n",
      "----------\n",
      "train Loss: 0.1347 Acc: 0.9454\n",
      "val Loss: 0.2229 Acc: 0.9280\n",
      "\n",
      "Epoch 266/299\n",
      "----------\n",
      "train Loss: 0.1302 Acc: 0.9497\n",
      "val Loss: 0.2132 Acc: 0.9349\n",
      "\n",
      "Epoch 267/299\n",
      "----------\n",
      "train Loss: 0.1297 Acc: 0.9499\n",
      "val Loss: 0.1960 Acc: 0.9372\n",
      "\n",
      "Epoch 268/299\n",
      "----------\n",
      "train Loss: 0.1320 Acc: 0.9497\n",
      "val Loss: 0.2223 Acc: 0.9335\n",
      "\n",
      "Epoch 269/299\n",
      "----------\n",
      "train Loss: 0.1285 Acc: 0.9497\n",
      "val Loss: 0.2101 Acc: 0.9322\n",
      "\n",
      "Epoch 270/299\n",
      "----------\n",
      "train Loss: 0.1335 Acc: 0.9472\n",
      "val Loss: 0.2180 Acc: 0.9308\n",
      "\n",
      "Epoch 271/299\n",
      "----------\n",
      "train Loss: 0.1313 Acc: 0.9493\n",
      "val Loss: 0.1946 Acc: 0.9377\n",
      "\n",
      "Epoch 272/299\n",
      "----------\n",
      "train Loss: 0.1300 Acc: 0.9498\n",
      "val Loss: 0.1944 Acc: 0.9391\n",
      "\n",
      "Epoch 273/299\n",
      "----------\n",
      "train Loss: 0.1264 Acc: 0.9488\n",
      "val Loss: 0.2064 Acc: 0.9303\n",
      "\n",
      "Epoch 274/299\n",
      "----------\n",
      "train Loss: 0.1303 Acc: 0.9481\n",
      "val Loss: 0.2149 Acc: 0.9345\n",
      "\n",
      "Epoch 275/299\n",
      "----------\n",
      "train Loss: 0.1277 Acc: 0.9491\n",
      "val Loss: 0.1940 Acc: 0.9372\n",
      "\n",
      "Epoch 276/299\n",
      "----------\n",
      "train Loss: 0.1302 Acc: 0.9482\n",
      "val Loss: 0.2120 Acc: 0.9294\n",
      "\n",
      "Epoch 277/299\n",
      "----------\n",
      "train Loss: 0.1252 Acc: 0.9503\n",
      "val Loss: 0.1938 Acc: 0.9349\n",
      "\n",
      "Epoch 278/299\n",
      "----------\n",
      "train Loss: 0.1316 Acc: 0.9462\n",
      "val Loss: 0.2007 Acc: 0.9368\n",
      "\n",
      "Epoch 279/299\n",
      "----------\n",
      "train Loss: 0.1305 Acc: 0.9480\n",
      "val Loss: 0.1998 Acc: 0.9349\n",
      "\n",
      "Epoch 280/299\n",
      "----------\n",
      "train Loss: 0.1270 Acc: 0.9496\n",
      "val Loss: 0.2084 Acc: 0.9335\n",
      "\n",
      "Epoch 281/299\n",
      "----------\n",
      "train Loss: 0.1260 Acc: 0.9511\n",
      "val Loss: 0.2175 Acc: 0.9275\n",
      "\n",
      "Epoch 282/299\n",
      "----------\n",
      "train Loss: 0.1257 Acc: 0.9521\n",
      "val Loss: 0.2114 Acc: 0.9317\n",
      "\n",
      "Epoch 283/299\n",
      "----------\n",
      "train Loss: 0.1269 Acc: 0.9521\n",
      "val Loss: 0.2027 Acc: 0.9372\n",
      "\n",
      "Epoch 284/299\n",
      "----------\n",
      "train Loss: 0.1264 Acc: 0.9497\n",
      "val Loss: 0.2155 Acc: 0.9303\n",
      "\n",
      "Epoch 285/299\n",
      "----------\n",
      "train Loss: 0.1361 Acc: 0.9453\n",
      "val Loss: 0.2350 Acc: 0.9243\n",
      "\n",
      "Epoch 286/299\n",
      "----------\n",
      "train Loss: 0.1331 Acc: 0.9485\n",
      "val Loss: 0.2323 Acc: 0.9262\n",
      "\n",
      "Epoch 287/299\n",
      "----------\n",
      "train Loss: 0.1292 Acc: 0.9481\n",
      "val Loss: 0.2185 Acc: 0.9335\n",
      "\n",
      "Epoch 288/299\n",
      "----------\n",
      "train Loss: 0.1308 Acc: 0.9483\n",
      "val Loss: 0.2107 Acc: 0.9340\n",
      "\n",
      "Epoch 289/299\n",
      "----------\n",
      "train Loss: 0.1305 Acc: 0.9483\n",
      "val Loss: 0.2155 Acc: 0.9363\n",
      "\n",
      "Epoch 290/299\n",
      "----------\n",
      "train Loss: 0.1279 Acc: 0.9492\n",
      "val Loss: 0.2084 Acc: 0.9280\n",
      "\n",
      "Epoch 291/299\n",
      "----------\n",
      "train Loss: 0.1309 Acc: 0.9490\n",
      "val Loss: 0.2285 Acc: 0.9349\n",
      "\n",
      "Epoch 292/299\n",
      "----------\n",
      "train Loss: 0.1249 Acc: 0.9519\n",
      "val Loss: 0.2055 Acc: 0.9312\n",
      "\n",
      "Epoch 293/299\n",
      "----------\n",
      "train Loss: 0.1268 Acc: 0.9493\n",
      "val Loss: 0.2043 Acc: 0.9349\n",
      "\n",
      "Epoch 294/299\n",
      "----------\n",
      "train Loss: 0.1275 Acc: 0.9507\n",
      "val Loss: 0.2084 Acc: 0.9372\n",
      "\n",
      "Epoch 295/299\n",
      "----------\n",
      "train Loss: 0.1273 Acc: 0.9508\n",
      "val Loss: 0.2132 Acc: 0.9345\n",
      "\n",
      "Epoch 296/299\n",
      "----------\n",
      "train Loss: 0.1252 Acc: 0.9500\n",
      "val Loss: 0.2075 Acc: 0.9349\n",
      "\n",
      "Epoch 297/299\n",
      "----------\n",
      "train Loss: 0.1272 Acc: 0.9499\n",
      "val Loss: 0.2246 Acc: 0.9308\n",
      "\n",
      "Epoch 298/299\n",
      "----------\n",
      "train Loss: 0.1275 Acc: 0.9492\n",
      "val Loss: 0.2230 Acc: 0.9289\n",
      "\n",
      "Epoch 299/299\n",
      "----------\n",
      "train Loss: 0.1304 Acc: 0.9492\n",
      "val Loss: 0.2306 Acc: 0.9322\n",
      "\n",
      "Training complete in 1m 48s\n",
      "Best val Acc: 0.941394\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.backends.cudnn.deterministic=True\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler,RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATA_PATH = \"../input/dry-beans-classification-iti-ai-pro-intake01/train.csv\"\n",
    "data = pd.read_csv(DATA_PATH)\n",
    "data.head()\n",
    "\n",
    "data=data.drop(\"ID\",axis=1)\n",
    "X = torch.Tensor(np.array(data.iloc[:, :-1], dtype=np.float32))\n",
    "label_encoder = LabelEncoder()\n",
    "standard_scaler = RobustScaler()\n",
    "out=label_encoder.fit_transform(data.iloc[:, -1])\n",
    "y = torch.Tensor(out)\n",
    "X = torch.Tensor(standard_scaler.fit_transform(X))\n",
    "y_label_classes = label_encoder.classes_\n",
    "n_samples = data.shape[0]\n",
    "\n",
    "X.shape\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(16,64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(32, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.fc1(x)\n",
    "        output = self.relu1(output)\n",
    "        output = self.fc2(output)\n",
    "        output = self.relu2(output)\n",
    "        output = self.fc3(output)\n",
    "        return output\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "train_data = TensorDataset(X_train, y_train.type(torch.LongTensor))\n",
    "test_data = TensorDataset(X_test, y_test.type(torch.LongTensor))\n",
    "\n",
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "\n",
    "batch_size = 64\n",
    "learning_rate = 0.02\n",
    "num_epoch = 300 #bs=32 epochs=300\n",
    "\n",
    "network = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    " \n",
    "# optimizer_ft = torch.optim.Adam(network.parameters(), lr=learning_rate)\n",
    "optimizer_ft = torch.optim.SGD(network.parameters(), lr=0.02, momentum=0.9)\n",
    "if train_on_gpu:\n",
    "    network.cuda()\n",
    "\n",
    "data = {\"train\": train_data, \"val\": test_data}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(data[x], batch_size=batch_size, shuffle=True) for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(data[x]) for x in ['train', 'val']}\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_model(model, criterion, optimizer, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "               # inputs = inputs.to(device)\n",
    "               # labels = labels.to(device)\n",
    "                inputs,labels=inputs.cuda(),labels.cuda()\n",
    "               # if train_on_gpu:\n",
    "            #data, target = data.cuda(), target.cuda()\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    torch.save(best_model_wts, \"savedmodel\")\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "\n",
    "model_ft = train_model(network, criterion, optimizer_ft, num_epochs=num_epoch)\n",
    "\n",
    "# preparing the test data\n",
    "\n",
    "test = pd.read_csv(\"../input/dry-beans-classification-iti-ai-pro-intake01/test.csv\")\n",
    "testcopy=test.copy()\n",
    "test=test.drop([\"ID\"],axis=1)\n",
    "Xt = torch.Tensor(np.array(test, dtype=np.float32))\n",
    "standard_scaler = RobustScaler()\n",
    "Xt = torch.Tensor(standard_scaler.fit_transform(Xt))\n",
    "n_samples =X.shape[0]\n",
    "\n",
    "# I converted to the model to evaluation mode\n",
    "model_ft.eval()\n",
    "\n",
    "model_ft.to(\"cpu\")\n",
    "\n",
    "#inputs,labels=dataloaders[\"train\"]\n",
    "out=model_ft(Xt)     # the output is a tensor\n",
    "_, preds = torch.max(out, 1)    #this takes the maximum probability for each output\n",
    "y_preds=np.array(preds)      # converts from tensor to numpy array\n",
    "\n",
    "y_pred_mlp=label_encoder.inverse_transform(y_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2d628e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-30T16:24:32.128282Z",
     "iopub.status.busy": "2021-09-30T16:24:32.127767Z",
     "iopub.status.idle": "2021-09-30T16:24:35.841504Z",
     "shell.execute_reply": "2021-09-30T16:24:35.841967Z",
     "shell.execute_reply.started": "2021-09-30T16:17:26.451607Z"
    },
    "papermill": {
     "duration": 3.809497,
     "end_time": "2021-09-30T16:24:35.842154",
     "exception": false,
     "start_time": "2021-09-30T16:24:32.032657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=64, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=64\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.7 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=64, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=64\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.7 will be ignored. Current value: feature_fraction=0.8\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"train.csv\"\n",
    "data_path=\"/kaggle/input/dry-beans-classification-iti-ai-pro-intake01/train.csv\"\n",
    "data_lgbm = pd.read_csv(data_path)\n",
    "\n",
    "label_encoder_lgbm = LabelEncoder()\n",
    "robust_scaler_lgbm = RobustScaler()\n",
    "\n",
    "out_lgbm=label_encoder_lgbm.fit_transform(data_lgbm.iloc[:, -1])\n",
    "y_label_classes_lgbm = label_encoder_lgbm.classes_\n",
    "n_samples_lgbm = data_lgbm.shape[0]\n",
    "\n",
    "X_lgbm=data_lgbm.drop([\"y\"],axis=1)\n",
    "X_lgbm=X_lgbm.drop(\"ID\",axis=1)\n",
    "\n",
    "y_lgbm=out_lgbm\n",
    "\n",
    "X_train_lgbm, X_test_lgbm, y_train_lgbm, y_test_lgbm = train_test_split(X_lgbm, y_lgbm, test_size=0.20, random_state=42)\n",
    "\n",
    "X_train_lgbm=X_train_lgbm.reset_index(drop=True)\n",
    "\n",
    "X_test_lgbm=X_test_lgbm.reset_index(drop=True)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def lgb_f1_score(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "    y_hat = np.round(y_hat) # scikits f1 doesn't like probabilities\n",
    "    return 'f1', f1_score(y_true, y_hat), True\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "params_lgbm={'num_leaves': 27,\n",
    " 'max_depth': 1,\n",
    " 'min_data_in_leaf': 64,\n",
    " 'learning_rate': 0.15,\n",
    " 'subsample': 0.9,\n",
    " 'feature_fraction': 0.8,\n",
    " 'n_estimators': 379,\n",
    " 'colsample_bytree': 0.7\n",
    "       }\n",
    "\n",
    "clf_lgbm = LGBMClassifier(**params_lgbm)\n",
    "\n",
    "clf_lgbm = clf_lgbm.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "y_pred_train_lgbm = clf_lgbm.predict(X_train)\n",
    "y_pred_test_lgbm =  clf_lgbm.predict(X_test)\n",
    "\n",
    "clf_lgbm = clf_lgbm.fit(X_lgbm, y_lgbm,)\n",
    "y_pred_train_lgbm = clf_lgbm.predict(data_lgbm.drop([\"y\",'ID'],axis=1))\n",
    "y_pred_test_lgbm =  clf_lgbm.predict(X_test_lgbm)\n",
    "\n",
    "test_path=\"/kaggle/input/dry-beans-classification-iti-ai-pro-intake01/test.csv\"\n",
    "test_lgbm = pd.read_csv(test_path)\n",
    "testcopy_lgbm=test_lgbm.copy()\n",
    "test_lgbm=test_lgbm.drop(\"ID\",axis=1)\n",
    "standard_scaler_lgbm = RobustScaler()\n",
    "\n",
    "y_preds_lgbm=clf_lgbm.predict(test_lgbm)    \n",
    "\n",
    "y_pred_lgbm=label_encoder_lgbm.inverse_transform(y_preds_lgbm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37aeccf3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-30T16:24:36.017799Z",
     "iopub.status.busy": "2021-09-30T16:24:36.017037Z",
     "iopub.status.idle": "2021-09-30T16:24:36.129903Z",
     "shell.execute_reply": "2021-09-30T16:24:36.129491Z",
     "shell.execute_reply.started": "2021-09-30T16:19:10.208663Z"
    },
    "papermill": {
     "duration": 0.201795,
     "end_time": "2021-09-30T16:24:36.130016",
     "exception": false,
     "start_time": "2021-09-30T16:24:35.928221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "new=pd.read_csv(\"../input/dry-beans-classification-iti-ai-pro-intake01/test.csv\")\n",
    "\n",
    "new['yLGBM'] = y_pred_lgbm\n",
    "new['yCAT'] = y_pred_cat\n",
    "new['yMLP'] = y_pred_mlp\n",
    "new['y'] = stats.mode(new[['yLGBM','yCAT','yMLP']],axis=1).mode\n",
    "new[['ID', 'y']].to_csv('new.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f30c3c09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-30T16:24:36.301723Z",
     "iopub.status.busy": "2021-09-30T16:24:36.301060Z",
     "iopub.status.idle": "2021-09-30T16:24:36.323896Z",
     "shell.execute_reply": "2021-09-30T16:24:36.324339Z",
     "shell.execute_reply.started": "2021-09-30T16:19:17.840302Z"
    },
    "papermill": {
     "duration": 0.11048,
     "end_time": "2021-09-30T16:24:36.324477",
     "exception": false,
     "start_time": "2021-09-30T16:24:36.213997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Area</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>MajorAxisLength</th>\n",
       "      <th>MinorAxisLength</th>\n",
       "      <th>AspectRation</th>\n",
       "      <th>Eccentricity</th>\n",
       "      <th>ConvexArea</th>\n",
       "      <th>EquivDiameter</th>\n",
       "      <th>Extent</th>\n",
       "      <th>Solidity</th>\n",
       "      <th>roundness</th>\n",
       "      <th>Compactness</th>\n",
       "      <th>ShapeFactor1</th>\n",
       "      <th>ShapeFactor2</th>\n",
       "      <th>ShapeFactor3</th>\n",
       "      <th>ShapeFactor4</th>\n",
       "      <th>yLGBM</th>\n",
       "      <th>yCAT</th>\n",
       "      <th>yMLP</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10834</td>\n",
       "      <td>57659</td>\n",
       "      <td>955.434</td>\n",
       "      <td>387.757607</td>\n",
       "      <td>196.625782</td>\n",
       "      <td>1.972059</td>\n",
       "      <td>0.861896</td>\n",
       "      <td>60188</td>\n",
       "      <td>270.949661</td>\n",
       "      <td>0.620790</td>\n",
       "      <td>0.957982</td>\n",
       "      <td>0.793735</td>\n",
       "      <td>0.698760</td>\n",
       "      <td>0.006725</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>0.488266</td>\n",
       "      <td>0.962889</td>\n",
       "      <td>HOROZ</td>\n",
       "      <td>HOROZ</td>\n",
       "      <td>HOROZ</td>\n",
       "      <td>HOROZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10835</td>\n",
       "      <td>28772</td>\n",
       "      <td>630.362</td>\n",
       "      <td>229.990785</td>\n",
       "      <td>159.609367</td>\n",
       "      <td>1.440960</td>\n",
       "      <td>0.719993</td>\n",
       "      <td>29127</td>\n",
       "      <td>191.399185</td>\n",
       "      <td>0.767458</td>\n",
       "      <td>0.987812</td>\n",
       "      <td>0.909913</td>\n",
       "      <td>0.832204</td>\n",
       "      <td>0.007994</td>\n",
       "      <td>0.002365</td>\n",
       "      <td>0.692563</td>\n",
       "      <td>0.997956</td>\n",
       "      <td>DERMASON</td>\n",
       "      <td>DERMASON</td>\n",
       "      <td>DERMASON</td>\n",
       "      <td>DERMASON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10836</td>\n",
       "      <td>54677</td>\n",
       "      <td>911.022</td>\n",
       "      <td>308.853903</td>\n",
       "      <td>226.398571</td>\n",
       "      <td>1.364204</td>\n",
       "      <td>0.680198</td>\n",
       "      <td>55858</td>\n",
       "      <td>263.850182</td>\n",
       "      <td>0.753013</td>\n",
       "      <td>0.978857</td>\n",
       "      <td>0.827860</td>\n",
       "      <td>0.854288</td>\n",
       "      <td>0.005649</td>\n",
       "      <td>0.001856</td>\n",
       "      <td>0.729808</td>\n",
       "      <td>0.995607</td>\n",
       "      <td>BARBUNYA</td>\n",
       "      <td>BARBUNYA</td>\n",
       "      <td>BARBUNYA</td>\n",
       "      <td>BARBUNYA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10837</td>\n",
       "      <td>24827</td>\n",
       "      <td>578.304</td>\n",
       "      <td>214.192699</td>\n",
       "      <td>147.788172</td>\n",
       "      <td>1.449322</td>\n",
       "      <td>0.723831</td>\n",
       "      <td>25121</td>\n",
       "      <td>177.794033</td>\n",
       "      <td>0.716508</td>\n",
       "      <td>0.988297</td>\n",
       "      <td>0.932871</td>\n",
       "      <td>0.830066</td>\n",
       "      <td>0.008627</td>\n",
       "      <td>0.002526</td>\n",
       "      <td>0.689009</td>\n",
       "      <td>0.998596</td>\n",
       "      <td>DERMASON</td>\n",
       "      <td>DERMASON</td>\n",
       "      <td>DERMASON</td>\n",
       "      <td>DERMASON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10838</td>\n",
       "      <td>141850</td>\n",
       "      <td>1432.847</td>\n",
       "      <td>524.923471</td>\n",
       "      <td>346.910920</td>\n",
       "      <td>1.513136</td>\n",
       "      <td>0.750492</td>\n",
       "      <td>143680</td>\n",
       "      <td>424.981211</td>\n",
       "      <td>0.804617</td>\n",
       "      <td>0.987263</td>\n",
       "      <td>0.868240</td>\n",
       "      <td>0.809606</td>\n",
       "      <td>0.003701</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>0.655462</td>\n",
       "      <td>0.991803</td>\n",
       "      <td>BOMBAY</td>\n",
       "      <td>BOMBAY</td>\n",
       "      <td>BOMBAY</td>\n",
       "      <td>BOMBAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID    Area  Perimeter  MajorAxisLength  MinorAxisLength  AspectRation  \\\n",
       "0  10834   57659    955.434       387.757607       196.625782      1.972059   \n",
       "1  10835   28772    630.362       229.990785       159.609367      1.440960   \n",
       "2  10836   54677    911.022       308.853903       226.398571      1.364204   \n",
       "3  10837   24827    578.304       214.192699       147.788172      1.449322   \n",
       "4  10838  141850   1432.847       524.923471       346.910920      1.513136   \n",
       "\n",
       "   Eccentricity  ConvexArea  EquivDiameter    Extent  Solidity  roundness  \\\n",
       "0      0.861896       60188     270.949661  0.620790  0.957982   0.793735   \n",
       "1      0.719993       29127     191.399185  0.767458  0.987812   0.909913   \n",
       "2      0.680198       55858     263.850182  0.753013  0.978857   0.827860   \n",
       "3      0.723831       25121     177.794033  0.716508  0.988297   0.932871   \n",
       "4      0.750492      143680     424.981211  0.804617  0.987263   0.868240   \n",
       "\n",
       "   Compactness  ShapeFactor1  ShapeFactor2  ShapeFactor3  ShapeFactor4  \\\n",
       "0     0.698760      0.006725      0.000989      0.488266      0.962889   \n",
       "1     0.832204      0.007994      0.002365      0.692563      0.997956   \n",
       "2     0.854288      0.005649      0.001856      0.729808      0.995607   \n",
       "3     0.830066      0.008627      0.002526      0.689009      0.998596   \n",
       "4     0.809606      0.003701      0.000981      0.655462      0.991803   \n",
       "\n",
       "      yLGBM      yCAT      yMLP         y  \n",
       "0     HOROZ     HOROZ     HOROZ     HOROZ  \n",
       "1  DERMASON  DERMASON  DERMASON  DERMASON  \n",
       "2  BARBUNYA  BARBUNYA  BARBUNYA  BARBUNYA  \n",
       "3  DERMASON  DERMASON  DERMASON  DERMASON  \n",
       "4    BOMBAY    BOMBAY    BOMBAY    BOMBAY  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03318e6b",
   "metadata": {
    "papermill": {
     "duration": 0.096285,
     "end_time": "2021-09-30T16:24:36.507366",
     "exception": false,
     "start_time": "2021-09-30T16:24:36.411081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 219.602459,
   "end_time": "2021-09-30T16:24:38.173272",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-30T16:20:58.570813",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
